# Простой web crawler и поисковик

Этот проект представляет собой простой web crawler и поисковик, который обходит веб-страницы, индексирует их содержимое и предоставляет возможность осуществлять поиск по этим данным. Проект создан на языке программирования Python, с использованием базы данных SQLite для хранения информации.

Основные компоненты проекта
1. Crawler ([Crawler/crawler.py](Crawler/crawler.py))
Crawler предназначен для обхода веб-страниц и индексации их содержимого. Основные функции crawler:
- Получение HTML-кода веб-страницы.
- Парсинг HTML-кода для извлечения содержимого.
- Индексирование слов и их расположения на страницах.
- Хранение информации о посещенных страницах и связях между страницами.

2. Поисковик (Searcher)
Поисковик осуществляет поиск по заранее индексированным страницам. Он позволяет осуществлять поиск по двум словам одновременно и возвращает результаты, оцененные по различным метрикам.
Отображения результатов поиска в виде HTML-страницы.

Метрики, используемые для оценки результатов поиска:

- Частота встречаемости слов на страницах.
- Расстояние между словами на странице.
- Рейтинг PageRank.


Запуск проекта:

- В проекте использовалась версия Python 3.11.5.
- Установите необходимые зависимости, включая библиотеки sqlite3, requests, beautifulsoup4, prettytable.
- Запустить [crawler.py](Crawler/crawler.py), который создаст и заполнит базу данных.
- Используя созданную [crawler.py](Crawler/crawler.py) базу данных, осуществить поиск, запустив [searcher.py](Searcher/searcher.py).
